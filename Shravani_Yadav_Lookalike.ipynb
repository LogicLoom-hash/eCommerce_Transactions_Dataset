{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieJcW1qCn8Kk",
        "outputId": "108ff1bc-add1-406a-98ac-b6a531a90944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C0001': [('C0107', 1.0), ('C0190', 0.999), ('C0048', 0.999)], 'C0002': [('C0178', 0.991), ('C0159', 0.982), ('C0136', 0.978)], 'C0003': [('C0133', 1.0), ('C0052', 0.998), ('C0152', 0.997)], 'C0004': [('C0165', 0.989), ('C0169', 0.987), ('C0126', 0.987)], 'C0005': [('C0186', 0.998), ('C0007', 0.994), ('C0146', 0.994)], 'C0006': [('C0171', 0.996), ('C0187', 0.992), ('C0011', 0.979)], 'C0007': [('C0115', 0.995), ('C0005', 0.994), ('C0140', 0.993)], 'C0008': [('C0065', 0.991), ('C0024', 0.984), ('C0194', 0.983)], 'C0009': [('C0010', 0.994), ('C0062', 0.989), ('C0111', 0.986)], 'C0010': [('C0111', 0.997), ('C0103', 0.997), ('C0009', 0.994)], 'C0011': [('C0137', 0.999), ('C0153', 0.997), ('C0126', 0.997)], 'C0012': [('C0195', 0.998), ('C0181', 0.996), ('C0113', 0.995)], 'C0013': [('C0099', 0.998), ('C0108', 0.998), ('C0129', 0.958)], 'C0014': [('C0060', 0.991), ('C0166', 0.956), ('C0119', 0.939)], 'C0015': [('C0131', 0.993), ('C0125', 0.975), ('C0094', 0.951)], 'C0016': [('C0183', 1.0), ('C0072', 0.997), ('C0035', 0.996)], 'C0017': [('C0019', 0.985), ('C0041', 0.985), ('C0057', 0.983)], 'C0018': [('C0125', 0.992), ('C0131', 0.974), ('C0015', 0.944)], 'C0019': [('C0017', 0.985), ('C0121', 0.981), ('C0057', 0.975)], 'C0020': [('C0050', 0.986), ('C0026', 0.969), ('C0035', 0.957)]}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "customers = pd.read_csv(\"Customers.csv\")  # Customer profiles\n",
        "products = pd.read_csv(\"Products.csv\")  # Product details\n",
        "transactions = pd.read_csv(\"Transactions.csv\")  # Transaction history\n",
        "\n",
        "# Data preprocessing\n",
        "# Handle missing data\n",
        "customers = customers.dropna(subset=[\"CustomerID\", \"Region\"])  # Drop rows with missing essential info\n",
        "transactions = transactions.dropna(subset=[\"CustomerID\", \"TotalValue\", \"ProductID\"])  # Drop rows with missing essential info\n",
        "\n",
        "# Step 1: Merge transactions with product details\n",
        "transactions = pd.merge(transactions, products[[\"ProductID\", \"ProductName\", \"Category\", \"Price\"]],\n",
        "                        on=\"ProductID\", how=\"left\")\n",
        "\n",
        "# Aggregate transaction data: Calculate total transaction value, purchase frequency, and dominant product category\n",
        "transaction_features = transactions.groupby(\"CustomerID\").agg({\n",
        "    \"TotalValue\": \"mean\",\n",
        "    \"TransactionID\": \"count\",\n",
        "    \"Category\": lambda x: x.value_counts().idxmax(),\n",
        "    \"ProductID\": lambda x: x.nunique()  # Count distinct products purchased by the customer\n",
        "}).rename(columns={\"TotalValue\": \"AvgTransactionValue\",\n",
        "                   \"TransactionID\": \"PurchaseFrequency\",\n",
        "                   \"Category\": \"DominantProductCategory\",\n",
        "                   \"ProductID\": \"DistinctProductCount\"})\n",
        "\n",
        "# Merge transaction features with customer data\n",
        "data = pd.merge(customers, transaction_features, on=\"CustomerID\", how=\"left\")\n",
        "\n",
        "# Handle missing values in merged data\n",
        "data.fillna({\"AvgTransactionValue\": data[\"AvgTransactionValue\"].mean(),\n",
        "             \"PurchaseFrequency\": 0,\n",
        "             \"DominantProductCategory\": \"Unknown\",\n",
        "             \"DistinctProductCount\": 0}, inplace=True)\n",
        "\n",
        "# Encode categorical features (e.g., region, product category)\n",
        "data = pd.get_dummies(data, columns=[\"Region\", \"DominantProductCategory\"], drop_first=True)\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = MinMaxScaler()\n",
        "numerical_features = [\"AvgTransactionValue\", \"PurchaseFrequency\", \"DistinctProductCount\"]\n",
        "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
        "\n",
        "# Compute similarity\n",
        "# Step 2: Select relevant features for similarity computation\n",
        "feature_columns = [\"AvgTransactionValue\", \"PurchaseFrequency\", \"DistinctProductCount\"] + \\\n",
        "                  [col for col in data.columns if \"Region_\" in col or \"DominantProductCategory_\" in col]\n",
        "\n",
        "# Calculate pairwise similarity\n",
        "similarity_matrix = cosine_similarity(data[feature_columns])\n",
        "\n",
        "# Step 3: Generate Lookalike Recommendations\n",
        "# Store top 3 lookalikes for each customer\n",
        "lookalike_map = {}\n",
        "\n",
        "for i in range(min(20, len(data))):  # For first 20 customers or less if there are fewer than 20\n",
        "    customer_id = data.iloc[i][\"CustomerID\"]\n",
        "    # Get similarity scores for this customer\n",
        "    scores = list(enumerate(similarity_matrix[i]))\n",
        "    # Sort by similarity score (excluding self-comparison)\n",
        "    scores = sorted(scores, key=lambda x: x[1], reverse=True)[1:4]  # Top 3 lookalikes\n",
        "    # Extract customer IDs and similarity scores\n",
        "    lookalikes = [(data.iloc[j][\"CustomerID\"], round(score, 3)) for j, score in scores]\n",
        "    lookalike_map[customer_id] = lookalikes\n",
        "\n",
        "# Save to Lookalike.csv\n",
        "lookalike_df = pd.DataFrame({\n",
        "    \"CustomerID\": lookalike_map.keys(),\n",
        "    \"Lookalikes\": [str(v) for v in lookalike_map.values()]\n",
        "})\n",
        "lookalike_df.to_csv(\"Lookalike.csv\", index=False)\n",
        "\n",
        "# Output the lookalike map for the first 20 customers\n",
        "print(lookalike_map)\n"
      ]
    }
  ]
}